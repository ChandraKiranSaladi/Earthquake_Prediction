{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import psutil\n",
    "import gc\n",
    "from catboost import CatBoostRegressor\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Read the csv file to take input into train dataframe using pandas, \n",
    "# 2 attributes from the training data: acoustic data: records the seismic activity and\n",
    "# time_to_failure: time left for the next laboratory earthquake\n",
    "train_dataset = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2b369f9462919c0c6e29f8a5b6183cb3c57f057a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e508c7f0c745b29d7fa9577ab4dc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4194), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# number of instances for each segment to be specified as 150000 because each test segment has 150000 observations.\n",
    "instances = 150000\n",
    "segments = int(np.floor(train_dataset.shape[0] / instances))\n",
    "\n",
    "# X_trainset is the training data: acoustic data\n",
    "# y_trainset is the value to be predicted, that is the time left for the next lab earthquake\n",
    "X_trainset = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "y_trainset = pd.DataFrame(index=range(segments), dtype=np.float64,\n",
    "                       columns=['time_to_failure'])\n",
    "\n",
    "# for every segment id, do feature engineering\n",
    "for segment in tqdm_notebook(range(segments)):\n",
    "\n",
    "#   creating segments of size 150000 starting from the row of segment id and upto segment id + instances\n",
    "    each_seg = train_dataset.iloc[segment * instances : segment * instances + instances]\n",
    "#     create x and y having acoustic_data and time_to _failure respectively\n",
    "    x_rawdata = each_seg['acoustic_data']\n",
    "    x = x_rawdata.values\n",
    "    y = each_seg['time_to_failure'].values[-1]\n",
    "    \n",
    "#     y train data is the time_to_failure for that segment instance\n",
    "    y_trainset.loc[segment, 'time_to_failure'] = y\n",
    "    X_trainset.loc[segment, 'average'] = x.mean()   # average of all acoustic data values for segment instance\n",
    "    X_trainset.loc[segment, 'standard_deviation'] = x.std()    # standard deviation\n",
    "    X_trainset.loc[segment, 'maximum'] = x.max()    # maximum value\n",
    "    X_trainset.loc[segment, 'minimum'] = x.min()    # minimum value\n",
    "    X_trainset.loc[segment, 'quantile_1_percentile'] = np.quantile(x,0.01)    # the value below which 1% of data appears in the acoustic_data attribute\n",
    "    X_trainset.loc[segment, 'quantile_5_percentile'] = np.quantile(x,0.05)    # the value below which 5%\n",
    "    X_trainset.loc[segment, 'quantile_95_percentile'] = np.quantile(x,0.95)    # the value below which 95%\n",
    "    X_trainset.loc[segment, 'quantile_99_percentile'] = np.quantile(x,0.99)    # the value below which 99%\n",
    "    X_trainset.loc[segment, 'median_absolute'] = np.median(np.abs(x))        # median of absolute values of acoustic_data\n",
    "    X_trainset.loc[segment, 'quantile_95_percentile_absolute'] = np.quantile(np.abs(x),0.95)    # the absolute value below which 95% of absolute acoustic_data data \n",
    "    X_trainset.loc[segment, 'quantile_99_percentile_absolute'] = np.quantile(np.abs(x),0.99)    # the absolute value below which 99% of absolute acoustic_data data \n",
    "    \n",
    "#     divide the data into group of 5; each of size 30000 and perform ANOVA tests to check\n",
    "#     if thesse groups have same population mean hence helping us determine the variance of the data.\n",
    "    X_trainset.loc[segment, 'F_test_measure'], X_trainset.loc[segment, 'p_test_measure'] = stats.f_oneway(x[:30000],x[30000:60000],x[60000:90000],x[90000:120000],x[120000:])\n",
    "\n",
    "#     .diff will give the change in x with respect to it's previous value; mean of all such changes.\n",
    "    X_trainset.loc[segment, 'average_change_absolute'] = np.mean(np.diff(x))      \n",
    "    \n",
    "#     take change values and divide by itself, then consider only those which come out to be non-zero\n",
    "    X_trainset.loc[segment, 'average_change_rate'] = np.mean(np.nonzero((np.diff(x) / x[:-1]))[0])\n",
    "    X_trainset.loc[segment, 'maximum_absolute'] = np.abs(x).max()     # max of absolute values in acoustic_data\n",
    "    \n",
    "    for windows in [10,100]:\n",
    "        x_roll_std = x_rawdata.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = x_rawdata.rolling(windows).mean().dropna().values\n",
    "        \n",
    "        X_trainset.loc[segment, 'average_rolling_standard_deviation' + str(windows)] = x_roll_std.mean()\n",
    "        X_trainset.loc[segment, 'standard_deviation_rolling_standard_deviation' + str(windows)] = x_roll_std.std()\n",
    "        X_trainset.loc[segment, 'maximum_rolling_standard_deviation' + str(windows)] = x_roll_std.max()\n",
    "        X_trainset.loc[segment, 'minimum_rolling_standard_deviation' + str(windows)] = x_roll_std.min()\n",
    "        X_trainset.loc[segment, 'quantile_1_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.01)\n",
    "        X_trainset.loc[segment, 'quantile_5_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.05)\n",
    "        X_trainset.loc[segment, 'quantile_95_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.95)\n",
    "        X_trainset.loc[segment, 'quantile_99_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.99)\n",
    "        X_trainset.loc[segment, 'average_change_absolute_rolling_standard_deviation' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "        X_trainset.loc[segment, 'average_change_rate_rolling_standard_deviation' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "        X_trainset.loc[segment, 'maximum_absolute_rolling_standard_deviation' + str(windows)] = np.abs(x_roll_std).max()\n",
    "        \n",
    "        X_trainset.loc[segment, 'average_absolute_rolling_mean' + str(windows)] = x_roll_mean.mean()\n",
    "        X_trainset.loc[segment, 'standard_deviation_rolling_mean' + str(windows)] = x_roll_mean.std()\n",
    "        X_trainset.loc[segment, 'maximum_rolling_mean' + str(windows)] = x_roll_mean.max()\n",
    "        X_trainset.loc[segment, 'minimum_rolling_mean' + str(windows)] = x_roll_mean.min()\n",
    "        X_trainset.loc[segment, 'quantile_1_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.01)\n",
    "        X_trainset.loc[segment, 'quantile_5_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.05)\n",
    "        X_trainset.loc[segment, 'quantile_95_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.95)\n",
    "        X_trainset.loc[segment, 'quantile_99_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.99)\n",
    "        X_trainset.loc[segment, 'average_change_absolute_rolling_mean' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "        X_trainset.loc[segment, 'average_change_rate_rolling_mean' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "        X_trainset.loc[segment, 'maximum_absolute_rolling_mean' + str(windows)] = np.abs(x_roll_mean).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "8a8fefba8af693620aa3598ac3de8f2ad521bbf4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eabb6795cdd480ab9a1a31e1bd113d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2624), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:53: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1584x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "X_testset = pd.DataFrame(columns=X_trainset.columns, dtype=np.float64, index=submission.index)\n",
    "\n",
    "for i, seg_id in enumerate(tqdm_notebook(X_testset.index)):\n",
    "    each_seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    \n",
    "    x_rawdata = each_seg['acoustic_data']\n",
    "    x_roll = x_rawdata.rolling(windows).std().dropna().values\n",
    "    x = x_rawdata.values\n",
    "    \n",
    "    X_testset.loc[seg_id, 'average'] = x.mean()\n",
    "    X_testset.loc[seg_id, 'standard_deviation'] = x.std()\n",
    "    X_testset.loc[seg_id, 'maximum'] = x.max()\n",
    "    X_testset.loc[seg_id, 'minimum'] = x.min()\n",
    "    X_testset.loc[seg_id, 'quantile_1_percentile'] = np.quantile(x,0.01)\n",
    "    X_testset.loc[seg_id, 'quantile_5_percentile'] = np.quantile(x,0.05)\n",
    "    X_testset.loc[seg_id, 'quantile_95_percentile'] = np.quantile(x,0.95)\n",
    "    X_testset.loc[seg_id, 'quantile_99_percentile'] = np.quantile(x,0.99)\n",
    "    X_testset.loc[seg_id, 'median_absolute'] = np.median(np.abs(x))\n",
    "    X_testset.loc[seg_id, 'quantile_95_percentile_absolute'] = np.quantile(np.abs(x),0.95)\n",
    "    X_testset.loc[seg_id, 'quantile_99_percentile_absolute'] = np.quantile(np.abs(x),0.99)\n",
    "    X_testset.loc[seg_id, 'F_test_measure'], X_trainset.loc[segment, 'p_test_measure'] = stats.f_oneway(x[:30000],x[30000:60000],x[60000:90000],x[90000:120000],x[120000:])\n",
    "    X_testset.loc[seg_id, 'average_change_absolute'] = np.mean(np.diff(x))\n",
    "    X_testset.loc[seg_id, 'average_change_rate'] = np.mean(np.nonzero((np.diff(x) / x[:-1]))[0])\n",
    "    X_testset.loc[seg_id, 'maximum_absolute'] = np.abs(x).max()\n",
    "    \n",
    "    for windows in [10,100]:\n",
    "        x_roll_std = x_rawdata.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = x_rawdata.rolling(windows).mean().dropna().values\n",
    "        \n",
    "        X_testset.loc[seg_id, 'average_rolling_standard_deviation' + str(windows)] = x_roll_std.mean()\n",
    "        X_testset.loc[seg_id, 'standard_deviation_rolling_standard_deviation' + str(windows)] = x_roll_std.std()\n",
    "        X_testset.loc[seg_id, 'maximum_rolling_standard_deviation' + str(windows)] = x_roll_std.max()\n",
    "        X_testset.loc[seg_id, 'minimum_rolling_standard_deviation' + str(windows)] = x_roll_std.min()\n",
    "        X_testset.loc[seg_id, 'quantile_1_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.01)\n",
    "        X_testset.loc[seg_id, 'quantile_5_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.05)\n",
    "        X_testset.loc[seg_id, 'quantile_95_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.95)\n",
    "        X_testset.loc[seg_id, 'quantile_99_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.99)\n",
    "        X_testset.loc[seg_id, 'average_change_absolute_rolling_standard_deviation' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "        X_testset.loc[seg_id, 'average_change_rate_rolling_standard_deviation' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "        X_testset.loc[seg_id, 'maximum_absolute_rolling_standard_deviation' + str(windows)] = np.abs(x_roll_std).max()\n",
    "        \n",
    "        X_testset.loc[seg_id, 'average_absolute_rolling_mean' + str(windows)] = x_roll_mean.mean()\n",
    "        X_testset.loc[seg_id, 'standard_deviation_rolling_mean' + str(windows)] = x_roll_mean.std()\n",
    "        X_testset.loc[seg_id, 'maximum_rolling_mean' + str(windows)] = x_roll_mean.max()\n",
    "        X_testset.loc[seg_id, 'minimum_rolling_mean' + str(windows)] = x_roll_mean.min()\n",
    "        X_testset.loc[seg_id, 'quantile_1_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.01)\n",
    "        X_testset.loc[seg_id, 'quantile_5_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.05)\n",
    "        X_testset.loc[seg_id, 'quantile_95_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.95)\n",
    "        X_testset.loc[seg_id, 'quantile_99_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.99)\n",
    "        X_testset.loc[seg_id, 'average_change_absolute_rolling_mean' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "        X_testset.loc[seg_id, 'average_change_rate_rolling_mean' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "        X_testset.loc[seg_id, 'maximum_absolute_rolling_mean' + str(windows)] = np.abs(x_roll_mean).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_trainset)\n",
    "X_trainset = pd.DataFrame(scaler.transform(X_trainset), columns=X_trainset.columns)\n",
    "X_testset = pd.DataFrame(scaler.transform(X_testset), columns=X_testset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "9172a3903b178e151d8aefbc68e01426a39e9e2e"
   },
   "outputs": [],
   "source": [
    "# Specification of K_Fold Cross Validation technique \n",
    "# Using K_Fold cross validation due to the absence of a singular continuous split of train and test\n",
    "num_folds = 5\n",
    "k_folds = KFold(n_splits=num_folds, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the LightGB Model\n",
    "# @params:\tX -\tX_train data\n",
    "# \t\t\tX_testset - X_test data\n",
    "# \t\t\ty - y_train data\n",
    "# \t\t\tparams - The parameters to fit the model into LGBM\n",
    "# \t\t\tk_folds - Number of folds for Cross validation\n",
    "# \t\t\tmodel - Specify the model to be fit into\n",
    "# @return:\tx_values - The values against who to compare the predicted values\n",
    "# \t\t\tprediction - Predicted values of time to failure with respect to the segments in test dataset\n",
    "def train_model_lgb(X=X_trainset, X_testset=X_testset, y=y_trainset, params=None, k_folds=k_folds, model=None):\n",
    "\n",
    "  x_values = np.zeros(len(X))\n",
    "  prediction = np.zeros(len(X_testset))\n",
    "  scores = []\n",
    "  feature_importance = pd.DataFrame()\n",
    "  for fold_n, (trainset_index, valid_set_index) in enumerate(k_folds.split(X)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train_per_fold, X_valid_per_fold = X.iloc[trainset_index], X.iloc[valid_set_index]\n",
    "    y_train_per_fold, y_valid_per_fold = y.iloc[trainset_index], y.iloc[valid_set_index]\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "    model.fit(X_train_per_fold, y_train_per_fold, \n",
    "           eval_set=[(X_train_per_fold, y_train_per_fold), (X_valid_per_fold, y_valid_per_fold)], eval_metric='mae',\n",
    "           verbose=1000, early_stopping_rounds=200)\n",
    "\n",
    "    y_pred_valid = model.predict(X_valid_per_fold)\n",
    "    y_pred = model.predict(X_testset, num_iteration=model.best_iteration_)\n",
    "\n",
    "    x_values[valid_set_index] = y_pred_valid.reshape(-1,)\n",
    "    scores.append(mean_absolute_error(y_valid_per_fold, y_pred_valid))\n",
    "\n",
    "    prediction += y_pred\n",
    "\n",
    "  prediction /= num_folds\n",
    "  print('CV mean score: {0:.4f}.'.format(mean_absolute_error(y, x_values)))\n",
    "  return x_values, prediction\n",
    "\n",
    "\n",
    "# Function to train the XtremeGB Model\n",
    "# @params:\tX -\tX_train data\n",
    "# \t\t\tX_testset - X_test data\n",
    "# \t\t\ty - y_train data\n",
    "# \t\t\tparams - The parameters to fit the model into XGBM\n",
    "# \t\t\tk_folds - Number of folds for Cross validation\n",
    "# \t\t\tmodel - Specify the model to be fit into\n",
    "# @return:\tx_values - The values against who to compare the predicted values\n",
    "# \t\t\tprediction - Predicted values of time to failure with respect to the segments in test dataset\n",
    "def train_model_xgb(X=X_trainset, X_testset=X_testset, y=y_trainset, params=None, k_folds=k_folds, model=None):\n",
    "\n",
    "  x_value = np.zeros(len(X))\n",
    "  prediction = np.zeros(len(X_testset))\n",
    "  scores = []\n",
    "  feature_importance = pd.DataFrame()\n",
    "  for fold_n, (trainset_index, valid_set_index) in enumerate(k_folds.split(X)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train_per_fold, X_valid_per_fold = X.iloc[trainset_index], X.iloc[valid_set_index]\n",
    "    y_train_per_fold, y_valid_per_fold = y.iloc[trainset_index], y.iloc[valid_set_index]\n",
    "\n",
    "    train_data = xgb.DMatrix(data=X_train_per_fold, label=y_train_per_fold, feature_names=X_trainset.columns)\n",
    "    valid_data = xgb.DMatrix(data=X_valid_per_fold, label=y_valid_per_fold, feature_names=X_trainset.columns)\n",
    "\n",
    "    watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "    model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "    y_pred_valid = model.predict(xgb.DMatrix(X_valid_per_fold, feature_names=X_trainset.columns), ntree_limit=model.best_ntree_limit)\n",
    "    y_pred = model.predict(xgb.DMatrix(X_testset, feature_names=X_trainset.columns), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "    x_value[valid_set_index] = y_pred_valid.reshape(-1,)\n",
    "    scores.append(mean_absolute_error(y_valid_per_fold, y_pred_valid))\n",
    "\n",
    "    prediction += y_pred\n",
    "\n",
    "  prediction /= num_folds\n",
    "  print('CV mean score: {0:.4f}.'.format(mean_absolute_error(y, x_value)))\n",
    "  return x_value, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "c532c79cb6f3417c3a9eec579979aaea91a0aa10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sat Apr 27 03:02:37 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 2.10575\tvalid_1's l1: 2.25612\n",
      "[2000]\ttraining's l1: 1.88719\tvalid_1's l1: 2.11666\n",
      "[3000]\ttraining's l1: 1.78589\tvalid_1's l1: 2.08789\n",
      "[4000]\ttraining's l1: 1.71563\tvalid_1's l1: 2.07877\n",
      "[5000]\ttraining's l1: 1.6579\tvalid_1's l1: 2.07478\n",
      "Early stopping, best iteration is:\n",
      "[5517]\ttraining's l1: 1.63158\tvalid_1's l1: 2.07403\n",
      "Fold 1 started at Sat Apr 27 03:02:55 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 2.11526\tvalid_1's l1: 2.21136\n",
      "[2000]\ttraining's l1: 1.90111\tvalid_1's l1: 2.0635\n",
      "[3000]\ttraining's l1: 1.79947\tvalid_1's l1: 2.03858\n",
      "Early stopping, best iteration is:\n",
      "[3740]\ttraining's l1: 1.7436\tvalid_1's l1: 2.03629\n",
      "Fold 2 started at Sat Apr 27 03:03:06 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 2.10314\tvalid_1's l1: 2.25662\n",
      "[2000]\ttraining's l1: 1.87857\tvalid_1's l1: 2.14795\n",
      "[3000]\ttraining's l1: 1.77376\tvalid_1's l1: 2.13525\n",
      "Early stopping, best iteration is:\n",
      "[3799]\ttraining's l1: 1.71435\tvalid_1's l1: 2.13281\n",
      "Fold 3 started at Sat Apr 27 03:03:18 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 2.14213\tvalid_1's l1: 2.05295\n",
      "[2000]\ttraining's l1: 1.92066\tvalid_1's l1: 1.92454\n",
      "[3000]\ttraining's l1: 1.81543\tvalid_1's l1: 1.9083\n",
      "Early stopping, best iteration is:\n",
      "[3360]\ttraining's l1: 1.78612\tvalid_1's l1: 1.90705\n",
      "Fold 4 started at Sat Apr 27 03:03:29 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 2.11507\tvalid_1's l1: 2.20111\n",
      "[2000]\ttraining's l1: 1.89374\tvalid_1's l1: 2.07556\n",
      "[3000]\ttraining's l1: 1.79111\tvalid_1's l1: 2.05441\n",
      "[4000]\ttraining's l1: 1.71726\tvalid_1's l1: 2.04849\n",
      "Early stopping, best iteration is:\n",
      "[4692]\ttraining's l1: 1.67428\tvalid_1's l1: 2.04706\n",
      "CV mean score: 2.0394.\n"
     ]
    }
   ],
   "source": [
    "# LGB Parameter specification to be passed on the the function call thereafter\n",
    "lgb_params = {'num_leaves': 64,\n",
    "         'min_data_in_leaf': 50,\n",
    "         'objective': 'mae',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.001,\n",
    "         \"boosting\": \"gbdt\",\n",
    "          \"feature_fraction\": 0.5,\n",
    "         \"bagging_freq\": 2,\n",
    "         \"bagging_fraction\": 0.5,\n",
    "         \"bagging_seed\": 0,\n",
    "         \"metric\": 'mae',\n",
    "         \"verbosity\": -1,\n",
    "         'reg_alpha': 1.0,\n",
    "         'reg_lambda': 1.0,\n",
    "         }\n",
    "x_value_lgb, prediction_lgb = train_model_lgb(params = lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "2d01a97d68c4031b3b2d11974a17b2e7053efc88",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sat Apr 27 02:58:02 2019\n",
      "[0]\ttrain-mae:5.13025\tvalid_data-mae:5.26969\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[242]\ttrain-mae:1.62971\tvalid_data-mae:2.1087\n",
      "\n",
      "Fold 1 started at Sat Apr 27 02:58:06 2019\n",
      "[0]\ttrain-mae:5.15714\tvalid_data-mae:5.16176\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[278]\ttrain-mae:1.60137\tvalid_data-mae:2.03861\n",
      "\n",
      "Fold 2 started at Sat Apr 27 02:58:10 2019\n",
      "[0]\ttrain-mae:5.15105\tvalid_data-mae:5.18888\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "[500]\ttrain-mae:1.38039\tvalid_data-mae:2.14713\n",
      "Stopping. Best iteration:\n",
      "[315]\ttrain-mae:1.53907\tvalid_data-mae:2.13704\n",
      "\n",
      "Fold 3 started at Sat Apr 27 02:58:14 2019\n",
      "[0]\ttrain-mae:5.19952\tvalid_data-mae:4.99284\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[278]\ttrain-mae:1.61984\tvalid_data-mae:1.94291\n",
      "\n",
      "Fold 4 started at Sat Apr 27 02:58:18 2019\n",
      "[0]\ttrain-mae:5.15295\tvalid_data-mae:5.18056\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[259]\ttrain-mae:1.62081\tvalid_data-mae:2.07509\n",
      "\n",
      "CV mean score: 2.0605.\n"
     ]
    }
   ],
   "source": [
    "# XGB Parameter specification to be passed on the the function call thereafter\n",
    "xgb_params = {'eta': 0.01,\n",
    "              'max_depth': 6,\n",
    "              'subsample': 0.8,\n",
    "              'colsample_bytree': 0.8,\n",
    "              'colsample_bylevel': 0.8,\n",
    "              'colsample_bynode': 0.8,\n",
    "              'lambda': 0.1,\n",
    "              'alpha' : 0.1,\n",
    "              'objective': 'reg:linear',\n",
    "              'eval_metric': 'mae',\n",
    "              'silent': True,\n",
    "              'nthread': 4}\n",
    "x_value_xgb, prediction_xgb = train_model_xgb(params=xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "6ed44e58c888d3891ece9bb8d01a5d56ae682eb1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.060464872800424\n"
     ]
    }
   ],
   "source": [
    "# Precision printing of the Mean Absolute Error value\n",
    "print(mean_absolute_error(y_trainset, (x_value_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "aed2c67cf5b7596b785bb85b0943ee9fef6e8187"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.21973922, 5.41623869, 5.276544  , 7.78150188, 6.81169566,\n",
       "       2.36501752, 7.000268  , 4.19822395, 4.78667879, 2.30686442])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 segments' time to failure values\n",
    "prediction_lgb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "acf417f3540ca69267b1088e199e4ae358b36439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              time_to_failure\n",
      "seg_id                       \n",
      "seg_00030f  2.911924505233765\n",
      "seg_0012b5  5.973187828063965\n",
      "seg_00184e  4.757333087921142\n",
      "seg_003339  7.688481330871582\n",
      "seg_0042cc  6.839030265808105\n"
     ]
    }
   ],
   "source": [
    "# Kaggle submission\n",
    "submission['time_to_failure'] = (prediction_xgb)\n",
    "print(submission.head())\n",
    "submission.to_csv('submission_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
